<!DOCTYPE html>
<html lang="en">

  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Recurrent Neural Networks in Tensorflow II | Verisk Blog</title>
<meta name="generator" content="Jekyll v4.2.1">
<meta property="og:title" content="Recurrent Neural Networks in Tensorflow II">
<meta property="og:locale" content="en_US">
<meta name="description" content="This is the second in a series of posts about recurrent neural networks in Tensorflow. The first post lives here. In this post, we will build upon our vanilla RNN by learning how to use Tensorflow’s scan and dynamic_rnn models, upgrading the RNN cell and stacking multiple RNNs, and adding dropout and layer normalization. We will then use our upgraded RNN to generate some text, character by character.">
<meta property="og:description" content="This is the second in a series of posts about recurrent neural networks in Tensorflow. The first post lives here. In this post, we will build upon our vanilla RNN by learning how to use Tensorflow’s scan and dynamic_rnn models, upgrading the RNN cell and stacking multiple RNNs, and adding dropout and layer normalization. We will then use our upgraded RNN to generate some text, character by character.">
<link rel="canonical" href="https://aadikuchlous.github.io/jekyll/tensorflow/neuralnetworks/2022/01/04/recurrent-neural-networks-in-tensorflow-ii.html">
<meta property="og:url" content="https://aadikuchlous.github.io/jekyll/tensorflow/neuralnetworks/2022/01/04/recurrent-neural-networks-in-tensorflow-ii.html">
<meta property="og:site_name" content="Verisk Blog">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2022-01-04T11:28:34+05:30">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Recurrent Neural Networks in Tensorflow II">
<script type="application/ld+json">
{"url":"https://aadikuchlous.github.io/jekyll/tensorflow/neuralnetworks/2022/01/04/recurrent-neural-networks-in-tensorflow-ii.html","headline":"Recurrent Neural Networks in Tensorflow II","dateModified":"2022-01-04T11:28:34+05:30","datePublished":"2022-01-04T11:28:34+05:30","description":"This is the second in a series of posts about recurrent neural networks in Tensorflow. The first post lives here. In this post, we will build upon our vanilla RNN by learning how to use Tensorflow’s scan and dynamic_rnn models, upgrading the RNN cell and stacking multiple RNNs, and adding dropout and layer normalization. We will then use our upgraded RNN to generate some text, character by character.","mainEntityOfPage":{"@type":"WebPage","@id":"https://aadikuchlous.github.io/jekyll/tensorflow/neuralnetworks/2022/01/04/recurrent-neural-networks-in-tensorflow-ii.html"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<!--
  <link rel="stylesheet" href="/assets/main.css">

  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
  -->

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous">

  <link rel="stylesheet" href="/assets/css/main.css">
<!--
  <link rel="stylesheet" href="/assets/css/bootstrap.css">
-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/mansory.js" type="text/javascript"></script>

  <script src="/assets/js/common.js"></script><link type="application/atom+xml" rel="alternate" href="https://aadikuchlous.github.io/feed.xml" title="Verisk Blog">
<link rel="icon" type="image/png" href="/assets/favicon.png">

<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<header class="site-header" role="banner">

  <div class="wrapper">
<a class="site-title" rel="author" href="/">Verisk Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

        <div class="trigger">
<a class="page-link" href="/about/">About Us</a><a class="page-link" href="/authors/">Authors</a><a class="page-link" href="/bib/">Bibliography</a><a class="page-link" href="/blog/">Blog</a><a class="page-link" href="/projects/">Projects</a>
</div>
      </nav>
</div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Recurrent Neural Networks in Tensorflow II</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2022-01-04T11:28:34+05:30" itemprop="datePublished">Jan 4, 2022
      </time>
          • <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">
                  <a href="/authors/amar/">Amar Viswanathan</a>
                </span>
              </span>
         
          • <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">
                  <a href="/authors/sindhu/">Sindhu Hegde</a>
                </span>
              </span>
         </p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>This is the second in a series of posts about recurrent neural networks in Tensorflow. The first post lives <a href="https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html">here</a>. In this post, we will build upon our vanilla RNN by learning how to use Tensorflow’s scan and dynamic_rnn models, upgrading the RNN cell and stacking multiple RNNs, and adding dropout and layer normalization. We will then use our upgraded RNN to generate some text, character by character.</p>

<p><strong>Note 3/14/2017:</strong> This tutorial is quite a bit deprecated by changes to the TF api. Leaving it up since it may still be useful, and most changes to the API are cosmetic (biggest change is that many of the RNN cells and functions are in the tf.contrib.rnn module). There was also a change to the ptb_iterator. A (slightly modified) copy of the old version which should work until I update this tutorial is uploaded <a href="https://gist.github.com/spitis/2dd1720850154b25d2cec58d4b75c4a0">here</a>.</p>

<h3 id="recap-of-our-model">Recap of our model</h3>

<p>In the last post, we built a very simple, no frills RNN that was quickly able to learn to solve the toy task we created for it.</p>

<p>Here is the formal statement of our model from last time:</p>

<p>$ S_t = \text{tanh}(W(X_t \ @ \ S_{t-1}) + b_s) $</p>

<p>$ P_t = \text{softmax}(US_t + b_p) $</p>

<p>where $ @ $ represents vector concatenation, $ X_t \in R^n $ is an input vector, $ W \in R^{d \times (n + d)}, \  b_s \in R^d, \ U \in R^{n \times d} $ is the size of the input and output vectors, and d is the size of the hidden state vector. At time step 0, $ S_{-1} $ (the initial state) is initialized as a vector of zeros.</p>

<h3 id="task-and-data">Task and data</h3>

<p>This time around we will be building a character-level language model to generate character sequences, a la Andrej Karpathy’s <a href="https://github.com/karpathy/char-rnn">char-rnn</a> (and see, e.g., a Tensorflow implementation by Sherjil Ozair <a href="https://github.com/sherjilozair/char-rnn-tensorflow">here</a>).</p>

<p>Why do something that’s already been done? Well, this is a much harder task than the toy model from last time. This model needs to handle long sequences and learn long time dependencies. That makes a great task for learning about adding features to our RNN, and seeing how our changes affect the results as we go.</p>

<p>To start, let’s create our data generator. We’ll use the tiny-shakespeare corpus as our data, though we could use any plain text file. We’ll choose to use all of the characters in the text file as our vocabulary, treating lowercase and capital letters are separate characters. In practice, there may be some advantage to forcing the network to use similar representations for capital and lowercase letters by using the same one-hot representations for each, plus a binary flag to indicate whether or not the letter is a capital. Additionally, it is likely a good idea to restrict the vocabulary (i.e., the set of characters) used, by replacing uncommon characters with an UNK token (like a square: □).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
Imports
"""</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">urllib.request</span>
<span class="kn">from</span> <span class="nn">tensorflow.models.rnn.ptb</span> <span class="kn">import</span> <span class="n">reader</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
Load and process data, utility functions
"""</span>

<span class="n">file_url</span> <span class="o">=</span> <span class="s">'https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt'</span>
<span class="n">file_name</span> <span class="o">=</span> <span class="s">'tinyshakespeare.txt'</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="n">file_name</span><span class="p">):</span>
    <span class="n">urllib</span><span class="p">.</span><span class="n">request</span><span class="p">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="n">file_url</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_name</span><span class="p">,</span><span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">raw_data</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Data length:"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">raw_data</span><span class="p">))</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">raw_data</span><span class="p">)</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">idx_to_vocab</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">))</span>
<span class="n">vocab_to_idx</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">idx_to_vocab</span><span class="p">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">idx_to_vocab</span><span class="p">.</span><span class="n">keys</span><span class="p">()))</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocab_to_idx</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">raw_data</span><span class="p">]</span>
<span class="k">del</span> <span class="n">raw_data</span>

<span class="k">def</span> <span class="nf">gen_epochs</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">reader</span><span class="p">.</span><span class="n">ptb_iterator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">reset_graph</span><span class="p">():</span>
    <span class="k">if</span> <span class="s">'sess'</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">()</span> <span class="ow">and</span> <span class="n">sess</span><span class="p">:</span>
        <span class="n">sess</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">reset_default_graph</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">train_network</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">num_steps</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">save</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">set_random_seed</span><span class="p">(</span><span class="mi">2345</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
        <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">initialize_all_variables</span><span class="p">())</span>
        <span class="n">training_losses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">gen_epochs</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)):</span>
            <span class="n">training_loss</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">steps</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">training_state</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="ow">in</span> <span class="n">epoch</span><span class="p">:</span>
                <span class="n">steps</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">g</span><span class="p">[</span><span class="s">'x'</span><span class="p">]:</span> <span class="n">X</span><span class="p">,</span> <span class="n">g</span><span class="p">[</span><span class="s">'y'</span><span class="p">]:</span> <span class="n">Y</span><span class="p">}</span>
                <span class="k">if</span> <span class="n">training_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="n">feed_dict</span><span class="p">[</span><span class="n">g</span><span class="p">[</span><span class="s">'init_state'</span><span class="p">]]</span> <span class="o">=</span> <span class="n">training_state</span>
                <span class="n">training_loss_</span><span class="p">,</span> <span class="n">training_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">([</span><span class="n">g</span><span class="p">[</span><span class="s">'total_loss'</span><span class="p">],</span>
                                                      <span class="n">g</span><span class="p">[</span><span class="s">'final_state'</span><span class="p">],</span>
                                                      <span class="n">g</span><span class="p">[</span><span class="s">'train_step'</span><span class="p">]],</span>
                                                             <span class="n">feed_dict</span><span class="p">)</span>
                <span class="n">training_loss</span> <span class="o">+=</span> <span class="n">training_loss_</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="s">"Average training loss for Epoch"</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="s">":"</span><span class="p">,</span> <span class="n">training_loss</span><span class="o">/</span><span class="n">steps</span><span class="p">)</span>
            <span class="n">training_losses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">training_loss</span><span class="o">/</span><span class="n">steps</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">save</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">g</span><span class="p">[</span><span class="s">'saver'</span><span class="p">].</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">save</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">training_losses</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Data length: 1115394
</code></pre></div></div>

<h3 id="using-tfscan-and-dynamic_rnn-to-speed-things-up">Using tf.scan and dynamic_rnn to speed things up</h3>

<p>Recall from <a href="https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html">last post</a> that we represented each duplicate tensor of our RNN (e.g., the rnn inputs, rnn outputs, the predictions and the loss) as a list of tensors:</p>

<p style="text-align: center;"><img src="https://r2rt.com/static/images/BasicRNNLabeled.png" alt=""></p>

<p>This worked quite well for our toy task, because our longest dependency was 7 steps back and we never really needed to backpropagate errors more than 10 steps. Even with a word-level RNN, using lists will probably be sufficient. See, e.g., my post on <a href="http://r2rt.com/styles-of-truncated-backpropagation.html">Styles of Truncated Backpropagation</a>, where I build a 40-step graph with no problems. But for a character-level model, 40 characters isn’t a whole lot. We might want to capture much longer dependencies. So let’s see what happens when we build a graph that is 200 time steps wide:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_basic_rnn_graph_with_list</span><span class="p">(</span>
    <span class="n">state_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">):</span>

    <span class="n">reset_graph</span><span class="p">()</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'input_placeholder'</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'labels_placeholder'</span><span class="p">)</span>

    <span class="n">x_one_hot</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
    <span class="n">rnn_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">squeeze_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">x_one_hot</span><span class="p">)]</span>

    <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">BasicRNNCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">)</span>
    <span class="n">init_state</span> <span class="o">=</span> <span class="n">cell</span><span class="p">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">rnn_outputs</span><span class="p">,</span> <span class="n">final_state</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">rnn_inputs</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">init_state</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">'softmax'</span><span class="p">):</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'W'</span><span class="p">,</span> <span class="p">[</span><span class="n">state_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">])</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'b'</span><span class="p">,</span> <span class="p">[</span><span class="n">num_classes</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">rnn_output</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span> <span class="k">for</span> <span class="n">rnn_output</span> <span class="ow">in</span> <span class="n">rnn_outputs</span><span class="p">]</span>

    <span class="n">y_as_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">squeeze_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">y</span><span class="p">)]</span>

    <span class="n">loss_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">ones</span><span class="p">([</span><span class="n">batch_size</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">)]</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">seq2seq</span><span class="p">.</span><span class="n">sequence_loss_by_example</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y_as_list</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">)</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
    <span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">).</span><span class="n">minimize</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span>
        <span class="n">init_state</span> <span class="o">=</span> <span class="n">init_state</span><span class="p">,</span>
        <span class="n">final_state</span> <span class="o">=</span> <span class="n">final_state</span><span class="p">,</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="n">total_loss</span><span class="p">,</span>
        <span class="n">train_step</span> <span class="o">=</span> <span class="n">train_step</span>
    <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">build_basic_rnn_graph_with_list</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"It took"</span><span class="p">,</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="s">"seconds to build the graph."</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>It took 5.626644849777222 seconds to build the graph.
</code></pre></div></div>

<p>It took over 5 seconds to build the graph of the most basic RNN model! This could bad… what happens when we move up to a 3-layer LSTM?</p>

<p>Below, we switch out the RNN cell for a Multi-layer LSTM cell. We’ll go over the details of how to do this in the next section.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_multilayer_lstm_graph_with_list</span><span class="p">(</span>
    <span class="n">state_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">):</span>

    <span class="n">reset_graph</span><span class="p">()</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'input_placeholder'</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'labels_placeholder'</span><span class="p">)</span>

    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'embedding_matrix'</span><span class="p">,</span> <span class="p">[</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">state_size</span><span class="p">])</span>
    <span class="n">rnn_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span>
                                <span class="n">num_steps</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">x</span><span class="p">))]</span>

    <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">MultiRNNCell</span><span class="p">([</span><span class="n">cell</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">init_state</span> <span class="o">=</span> <span class="n">cell</span><span class="p">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">rnn_outputs</span><span class="p">,</span> <span class="n">final_state</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">rnn_inputs</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">init_state</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">'softmax'</span><span class="p">):</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'W'</span><span class="p">,</span> <span class="p">[</span><span class="n">state_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">])</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'b'</span><span class="p">,</span> <span class="p">[</span><span class="n">num_classes</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">rnn_output</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span> <span class="k">for</span> <span class="n">rnn_output</span> <span class="ow">in</span> <span class="n">rnn_outputs</span><span class="p">]</span>

    <span class="n">y_as_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">squeeze_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">y</span><span class="p">)]</span>

    <span class="n">loss_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">ones</span><span class="p">([</span><span class="n">batch_size</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">)]</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">seq2seq</span><span class="p">.</span><span class="n">sequence_loss_by_example</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y_as_list</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">)</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
    <span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">).</span><span class="n">minimize</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span>
        <span class="n">init_state</span> <span class="o">=</span> <span class="n">init_state</span><span class="p">,</span>
        <span class="n">final_state</span> <span class="o">=</span> <span class="n">final_state</span><span class="p">,</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="n">total_loss</span><span class="p">,</span>
        <span class="n">train_step</span> <span class="o">=</span> <span class="n">train_step</span>
    <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">build_multilayer_lstm_graph_with_list</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"It took"</span><span class="p">,</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="s">"seconds to build the graph."</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>It took 25.640846967697144 seconds to build the graph.
</code></pre></div></div>

<p>Yikes, almost 30 seconds.</p>

<p>Now this isn’t that big of an issue for training, because we only need to build the graph once. It could be a big issue, however, if we need to build the graph multiple times at test time.</p>

<p>To get around this long compile time, Tensorflow allows us to create the graph at runtime. Here is a quick demonstration of the difference, using Tensorflow’s dynamic_rnn function:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_multilayer_lstm_graph_with_dynamic_rnn</span><span class="p">(</span>
    <span class="n">state_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">):</span>

    <span class="n">reset_graph</span><span class="p">()</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'input_placeholder'</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'labels_placeholder'</span><span class="p">)</span>

    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'embedding_matrix'</span><span class="p">,</span> <span class="p">[</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">state_size</span><span class="p">])</span>

    <span class="c1"># Note that our inputs are no longer a list, but a tensor of dims batch_size x num_steps x state_size
</span>    <span class="n">rnn_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">MultiRNNCell</span><span class="p">([</span><span class="n">cell</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">init_state</span> <span class="o">=</span> <span class="n">cell</span><span class="p">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">rnn_outputs</span><span class="p">,</span> <span class="n">final_state</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">dynamic_rnn</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">rnn_inputs</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">init_state</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">'softmax'</span><span class="p">):</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'W'</span><span class="p">,</span> <span class="p">[</span><span class="n">state_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">])</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'b'</span><span class="p">,</span> <span class="p">[</span><span class="n">num_classes</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>

    <span class="c1">#reshape rnn_outputs and y so we can get the logits in a single matmul
</span>    <span class="n">rnn_outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rnn_outputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">state_size</span><span class="p">])</span>
    <span class="n">y_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">rnn_outputs</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>

    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y_reshaped</span><span class="p">))</span>
    <span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">).</span><span class="n">minimize</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span>
        <span class="n">init_state</span> <span class="o">=</span> <span class="n">init_state</span><span class="p">,</span>
        <span class="n">final_state</span> <span class="o">=</span> <span class="n">final_state</span><span class="p">,</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="n">total_loss</span><span class="p">,</span>
        <span class="n">train_step</span> <span class="o">=</span> <span class="n">train_step</span>
    <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">build_multilayer_lstm_graph_with_dynamic_rnn</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"It took"</span><span class="p">,</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="s">"seconds to build the graph."</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>It took 0.5314393043518066 seconds to build the graph.
</code></pre></div></div>

<p>Much better. One would think that pushing the graph construction to execution time would cause execution of the graph to go slower, but in this case, using dynamic_rnn actually speeds things up:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">g</span> <span class="o">=</span> <span class="n">build_multilayer_lstm_graph_with_list</span><span class="p">()</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">train_network</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"It took"</span><span class="p">,</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="s">"seconds to train for 3 epochs."</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Average training loss for Epoch 0 : 3.53323210245
Average training loss for Epoch 1 : 3.31435756163
Average training loss for Epoch 2 : 3.21755325109
It took 117.78161263465881 seconds to train for 3 epochs.
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">g</span> <span class="o">=</span> <span class="n">build_multilayer_lstm_graph_with_dynamic_rnn</span><span class="p">()</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">train_network</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"It took"</span><span class="p">,</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="s">"seconds to train for 3 epochs."</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Average training loss for Epoch 0 : 3.55792756053
Average training loss for Epoch 1 : 3.3225021006
Average training loss for Epoch 2 : 3.28286816745
It took 96.69413661956787 seconds to train for 3 epochs.
</code></pre></div></div>

<p>It’s not a breeze to work through and understand the dynamic_rnn code (which lives <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py">here</a>), but we can obtain a similar result ourselves by using tf.scan (dynamic_rnn does not use scan). Scan runs just a tad slower than Tensorflow’s optimized code, but is easier to understand and write yourself.</p>

<p>Scan is a higher-order function that you might be familiar with if you’ve done any programming in OCaml, Haskell or the like. In general, it takes a function $ (f: (x_t, y_{t-1}) \mapsto y_t) $, a sequence $ ([x_0, x_1 \dots x_n]) $ and an initial value $ (y_{-1}) $ and returns a sequence $ ([y_0, y_1 \dots y_n]) $ according to the rule: $ y_t = f(x_t, y_{t-1}) $. In Tensorflow, scan treats the first dimension of a Tensor as the sequence. Thus, if fed a Tensor of shape [n, m, o] as the sequence, scan would unpack it into a sequence of n-tensors, each with shape [m, o]. You can learn more about Tensorflow’s scan <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functional_ops.md#tfscanfn-elems-initializernone-parallel_iterations10-back_proptrue-swap_memoryfalse-namenone-scan">here</a>.</p>

<p>Below, I use scan with an LSTM so as to compare to the dynamic_rnn using Tensorflow above. Because LSTMs store their state in a 2-tuple, and we’re using a 3-layer network, the scan function produces, as <code class="language-plaintext highlighter-rouge">final_states</code> below, a 3-tuple (one for each layer) of 2-tuples (one for each LSTM state), each of shape [num_steps, batch_size, state_size]. We need only the last state, which is why we unpack, slice and repack <code class="language-plaintext highlighter-rouge">final_states</code> to get <code class="language-plaintext highlighter-rouge">final_state</code> below.</p>

<p>Another thing to note is that scan produces rnn_outputs with shape [num_steps, batch_size, state_size], whereas the dynamic_rnn produces rnn_outputs with shape [batch_size, num_steps, state_size] (the first two dimensions are switched). Dynamic_rnn has the flexibility to switch this behavior, using the “time_major” argument. Tf.scan does not have this flexibility, which is why we transpose <code class="language-plaintext highlighter-rouge">rnn_inputs</code> and <code class="language-plaintext highlighter-rouge">y</code> in the code below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_multilayer_lstm_graph_with_scan</span><span class="p">(</span>
    <span class="n">state_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">):</span>

    <span class="n">reset_graph</span><span class="p">()</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'input_placeholder'</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'labels_placeholder'</span><span class="p">)</span>

    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'embedding_matrix'</span><span class="p">,</span> <span class="p">[</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">state_size</span><span class="p">])</span>

    <span class="n">rnn_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">MultiRNNCell</span><span class="p">([</span><span class="n">cell</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">init_state</span> <span class="o">=</span> <span class="n">cell</span><span class="p">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">rnn_outputs</span><span class="p">,</span> <span class="n">final_states</span> <span class="o">=</span> \
        <span class="n">tf</span><span class="p">.</span><span class="n">scan</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">cell</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="n">tf</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">rnn_inputs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]),</span>
                <span class="n">initializer</span><span class="o">=</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">state_size</span><span class="p">]),</span> <span class="n">init_state</span><span class="p">))</span>

    <span class="c1"># there may be a better way to do this:
</span>    <span class="n">final_state</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span>
                  <span class="n">tf</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">slice</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="p">[</span><span class="n">num_steps</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">state_size</span><span class="p">])),</span>
                  <span class="n">tf</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">slice</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="p">[</span><span class="n">num_steps</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">state_size</span><span class="p">])))</span>
                       <span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">final_states</span><span class="p">])</span>

    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">'softmax'</span><span class="p">):</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'W'</span><span class="p">,</span> <span class="p">[</span><span class="n">state_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">])</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'b'</span><span class="p">,</span> <span class="p">[</span><span class="n">num_classes</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>

    <span class="n">rnn_outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rnn_outputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">state_size</span><span class="p">])</span>
    <span class="n">y_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">y</span><span class="p">,[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">rnn_outputs</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>

    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y_reshaped</span><span class="p">))</span>
    <span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">).</span><span class="n">minimize</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span>
        <span class="n">init_state</span> <span class="o">=</span> <span class="n">init_state</span><span class="p">,</span>
        <span class="n">final_state</span> <span class="o">=</span> <span class="n">final_state</span><span class="p">,</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="n">total_loss</span><span class="p">,</span>
        <span class="n">train_step</span> <span class="o">=</span> <span class="n">train_step</span>
    <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">build_multilayer_lstm_graph_with_scan</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"It took"</span><span class="p">,</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="s">"seconds to build the graph."</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">train_network</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"It took"</span><span class="p">,</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="s">"seconds to train for 3 epochs."</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>It took 0.6475389003753662 seconds to build the graph.
Average training loss for Epoch 0 : 3.55362293501
Average training loss for Epoch 1 : 3.32045680079
Average training loss for Epoch 2 : 3.27433713688
It took 101.60246014595032 seconds to train for 3 epochs.
</code></pre></div></div>

<p>Using scan was only marginally slower than using dynamic_rnn, and gives us the flexibility and understanding to tweak the code if we ever need to (e.g., if for some reason we wanted to create a skip connection from the state at timestep t-2 to timestep t, it would be easy to do with scan).</p>

<h3 id="upgrading-the-rnn-cell">Upgrading the RNN cell</h3>

<p>Above, we seamlessly swapped out the BasicRNNCell we were using for a Multi-layered LSTM cell. This was possible because the RNN cells conform to a general structure: every RNN cell is a function of the current input, $ X_t $, and the prior state, $ S_{t-1} $, that outputs a current state, $ S_{t} $, and a current output, $ Y_t $. Thus, in the same way that we can swap out activation functions in a feedforward net (e.g., change the tanh activation to a sigmoid or a relu activation), we can swap out the entire recurrence function (cell) in an RNN.</p>

<p>Note that while for basic RNN cells, the current output equals the current state $ (Y_t = S_t) $, this does not have to be the case. We’ll see how LSTMs and multi-layered RNNs diverge from this below.</p>

<p>Two popular choices for RNN cells are the GRU cell and the LSTM cell. By using gates, GRU and LSTM cells avoid the vanishing gradient problem and allow the network to learn longer-term dependencies. Their internals are quite complicated, and I would refer you to my post <a href="https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html">Written Memories: Understanding, Deriving and Extending the LSTM</a> for a good starting point to learn about them.</p>

<p>All we have to do to upgrade our vanilla RNN cell is to replace this line:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">BasicRNNCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">)</span>
</code></pre></div></div>

<p>with this for LSTM:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">)</span>
</code></pre></div></div>

<p>or this for GRU:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">GRUCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">)</span>
</code></pre></div></div>

<p>The LSTM keeps two sets of internal state vectors, $ c $ (for memory cell or constant error carousel) and $ h $ (for hidden state). By default, they are concatenated into a single vector, but as of this writing, using the default arguments to LSTMCell will produce a warning message:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>WARNING:tensorflow:&lt;tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7faade1708d0&gt;: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
</code></pre></div></div>

<p>This error tells us that it’s faster to represent the LSTM state as a tuple of $ c $ and $ h $, rather than as a concatenation of $ c $ and $ h $. You can tack on the argument <code class="language-plaintext highlighter-rouge">state_is_tuple=True</code> to have it do that.</p>

<p>By using a tuple for the state, we can also easily replace the base cell with a “MultiRNNCell” for multiple layers. To see why this works, consider that while a single cell:</p>

<p style="text-align: center;"><img src="https://r2rt.com/static/images/RNN_BasicRNNCell.png" alt=""></p>

<p>looks different from a two cells stacked on top of each other:</p>

<p style="text-align: center;"><img src="https://r2rt.com/static/images/RNN_MultiRNNCellUngrouped.png" alt=""></p>

<p>we can wrap the two cells into a single two-layer cell to make them look and behave as a single cell:</p>

<p style="text-align: center;"><img src="https://r2rt.com/static/images/RNN_MultiRNNCellGrouped.png" alt=""></p>

<p>To make this switch, we call <code class="language-plaintext highlighter-rouge">tf.nn.rnn_cell.MultiRNNCell</code>, which takes a list of RNNCells as its inputs and wraps them into a single cell:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">MultiRNNCell</span><span class="p">([</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">BasicRNNCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">)]</span> <span class="o">*</span> <span class="n">num_layers</span><span class="p">)</span>
</code></pre></div></div>

<p>Note that if you are wrapping an LSTMCell that uses <code class="language-plaintext highlighter-rouge">state_is_tuple=True</code>, you should pass this same argument to the MultiRNNCell as well.</p>

<h3 id="writing-a-custom-rnn-cell">Writing a custom RNN cell</h3>

<p>It’s almost too easy to use the standard GRU or LSTM cells, so let’s define our own RNN cell. Here’s a random idea that may or may not work: starting with a GRU cell, instead of taking a single transformation of its input, we enable it to take a weighted average of multiple transformations of its input. That is, using the notation from <a href="http://arxiv.org/pdf/1406.1078v3.pdf">Cho et al. (2014)</a>, instead of using $ Wx $ in our candidate state, $ \tilde h^{(t)} = \text{tanh}(Wx + U(r \odot h^{(t-1)}) $, we use a weighted average of $ W_1 x, \ W_2 x \dots W_n x $ for some $ n $. In other words, we will replace $ Wx $ with $ \Sigma\lambda_iW_ix $ for some weights $ \lambda_i $ that sum to $ 1 $. The vector of weights, $ \lambda $, will be calculated as $ \lambda = \text{softmax}(W_{avg}x^{(t)} + U_{avg}h^{(t-1)} + b) $. The idea is that we might benefit from treat the input differently in different scenarios (e.g., we may want to treat verbs differently than nouns).</p>

<p>To write the custom cell, we need to extend tf.nn.rnn_cell.RNNCell. Specifically, we need to fill in 3 abstract methods and write an <code class="language-plaintext highlighter-rouge">__init__</code> method (take a look at the Tensorflow code <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py">here</a>). First, let’s start with a GRU cell, adapted from Tensorflow’s implementation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GRUCell</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">RNNCell</span><span class="p">):</span>
    <span class="s">"""Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078)."""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_units</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_num_units</span> <span class="o">=</span> <span class="n">num_units</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">_num_units</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">_num_units</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">).</span><span class="n">__name__</span><span class="p">):</span>  <span class="c1"># "GRUCell"
</span>            <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">"Gates"</span><span class="p">):</span>  <span class="c1"># Reset gate and update gate.
</span>                <span class="c1"># We start with bias of 1.0 to not reset and not update.
</span>                <span class="n">ru</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">_linear</span><span class="p">([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">],</span>
                                        <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
                <span class="n">ru</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">ru</span><span class="p">)</span>
                <span class="n">r</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">ru</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">"Candidate"</span><span class="p">):</span>
                <span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">_linear</span><span class="p">([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">r</span> <span class="o">*</span> <span class="n">state</span><span class="p">],</span>
                                             <span class="bp">self</span><span class="p">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="bp">True</span><span class="p">))</span>
            <span class="n">new_h</span> <span class="o">=</span> <span class="n">u</span> <span class="o">*</span> <span class="n">state</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">u</span><span class="p">)</span> <span class="o">*</span> <span class="n">c</span>
        <span class="k">return</span> <span class="n">new_h</span><span class="p">,</span> <span class="n">new_h</span>
</code></pre></div></div>

<p>We modify the <code class="language-plaintext highlighter-rouge">__init__</code> method to take a parameter n at initialization, which will determine the number of transformation matrices $ W_i $ it will create:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_units</span><span class="p">,</span> <span class="n">num_weights</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">_num_units</span> <span class="o">=</span> <span class="n">num_units</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">_num_weights</span> <span class="o">=</span> <span class="n">num_weights</span>
</code></pre></div></div>

<p>Then, we modify the <code class="language-plaintext highlighter-rouge">Candidate</code> variable scope of the <code class="language-plaintext highlighter-rouge">__call__</code> method to do a weighted average as shown below (note that all of the $ W_i $ matrices are created as a single variable and then split into multiple tensors):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CustomCell</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">RNNCell</span><span class="p">):</span>
    <span class="s">"""Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078)."""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_units</span><span class="p">,</span> <span class="n">num_weights</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_num_units</span> <span class="o">=</span> <span class="n">num_units</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_num_weights</span> <span class="o">=</span> <span class="n">num_weights</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">_num_units</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">_num_units</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">).</span><span class="n">__name__</span><span class="p">):</span>  <span class="c1"># "GRUCell"
</span>            <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">"Gates"</span><span class="p">):</span>  <span class="c1"># Reset gate and update gate.
</span>                <span class="c1"># We start with bias of 1.0 to not reset and not update.
</span>                <span class="n">ru</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">_linear</span><span class="p">([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">],</span>
                                        <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
                <span class="n">ru</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">ru</span><span class="p">)</span>
                <span class="n">r</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">ru</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">"Candidate"</span><span class="p">):</span>
                <span class="n">lambdas</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">_linear</span><span class="p">([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">_num_weights</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>
                <span class="n">lambdas</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">_num_weights</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">lambdas</span><span class="p">))</span>

                <span class="n">Ws</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">"Ws"</span><span class="p">,</span>
                        <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">_num_weights</span><span class="p">,</span> <span class="n">inputs</span><span class="p">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">_num_units</span><span class="p">])</span>
                <span class="n">Ws</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">_num_weights</span><span class="p">,</span> <span class="n">Ws</span><span class="p">)]</span>

                <span class="n">candidate_inputs</span> <span class="o">=</span> <span class="p">[]</span>

                <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">W</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">Ws</span><span class="p">):</span>
                    <span class="n">candidate_inputs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">*</span> <span class="n">lambdas</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>

                <span class="n">Wx</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">add_n</span><span class="p">(</span><span class="n">candidate_inputs</span><span class="p">)</span>

                <span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">Wx</span> <span class="o">+</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">_linear</span><span class="p">([</span><span class="n">r</span> <span class="o">*</span> <span class="n">state</span><span class="p">],</span>
                                            <span class="bp">self</span><span class="p">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s">"second"</span><span class="p">))</span>
            <span class="n">new_h</span> <span class="o">=</span> <span class="n">u</span> <span class="o">*</span> <span class="n">state</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">u</span><span class="p">)</span> <span class="o">*</span> <span class="n">c</span>
        <span class="k">return</span> <span class="n">new_h</span><span class="p">,</span> <span class="n">new_h</span>
</code></pre></div></div>

<p>Let’s see how the custom cell stacks up to a regular GRU cell (using <code class="language-plaintext highlighter-rouge">num_steps = 30</code>, since this performs much better than <code class="language-plaintext highlighter-rouge">num_steps = 200</code> after 5 epochs – can you see why that might happen?):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_multilayer_graph_with_custom_cell</span><span class="p">(</span>
    <span class="n">cell_type</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">num_weights_for_custom_cell</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">state_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">):</span>

    <span class="n">reset_graph</span><span class="p">()</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'input_placeholder'</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'labels_placeholder'</span><span class="p">)</span>

    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'embedding_matrix'</span><span class="p">,</span> <span class="p">[</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">state_size</span><span class="p">])</span>

    <span class="n">rnn_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">cell_type</span> <span class="o">==</span> <span class="s">'Custom'</span><span class="p">:</span>
        <span class="n">cell</span> <span class="o">=</span> <span class="n">CustomCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">num_weights_for_custom_cell</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">cell_type</span> <span class="o">==</span> <span class="s">'GRU'</span><span class="p">:</span>
        <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">GRUCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">cell_type</span> <span class="o">==</span> <span class="s">'LSTM'</span><span class="p">:</span>
        <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">BasicRNNCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">cell_type</span> <span class="o">==</span> <span class="s">'LSTM'</span><span class="p">:</span>
        <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">MultiRNNCell</span><span class="p">([</span><span class="n">cell</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">MultiRNNCell</span><span class="p">([</span><span class="n">cell</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span><span class="p">)</span>

    <span class="n">init_state</span> <span class="o">=</span> <span class="n">cell</span><span class="p">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">rnn_outputs</span><span class="p">,</span> <span class="n">final_state</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">dynamic_rnn</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">rnn_inputs</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">init_state</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">'softmax'</span><span class="p">):</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'W'</span><span class="p">,</span> <span class="p">[</span><span class="n">state_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">])</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'b'</span><span class="p">,</span> <span class="p">[</span><span class="n">num_classes</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>

    <span class="c1">#reshape rnn_outputs and y
</span>    <span class="n">rnn_outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rnn_outputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">state_size</span><span class="p">])</span>
    <span class="n">y_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">rnn_outputs</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>

    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y_reshaped</span><span class="p">))</span>
    <span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">).</span><span class="n">minimize</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span>
        <span class="n">init_state</span> <span class="o">=</span> <span class="n">init_state</span><span class="p">,</span>
        <span class="n">final_state</span> <span class="o">=</span> <span class="n">final_state</span><span class="p">,</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="n">total_loss</span><span class="p">,</span>
        <span class="n">train_step</span> <span class="o">=</span> <span class="n">train_step</span>
    <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">g</span> <span class="o">=</span> <span class="n">build_multilayer_graph_with_custom_cell</span><span class="p">(</span><span class="n">cell_type</span><span class="o">=</span><span class="s">'GRU'</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">train_network</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"It took"</span><span class="p">,</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="s">"seconds to train for 5 epochs."</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Average training loss for Epoch 0 : 2.92919953048
Average training loss for Epoch 1 : 2.35888109404
Average training loss for Epoch 2 : 2.21945820894
Average training loss for Epoch 3 : 2.12258511006
Average training loss for Epoch 4 : 2.05038544733
It took 284.6971204280853 seconds to train for 5 epochs.
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">g</span> <span class="o">=</span> <span class="n">build_multilayer_graph_with_custom_cell</span><span class="p">(</span><span class="n">cell_type</span><span class="o">=</span><span class="s">'Custom'</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">train_network</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"It took"</span><span class="p">,</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="s">"seconds to train for 5 epochs."</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Average training loss for Epoch 0 : 3.04418995892
Average training loss for Epoch 1 : 2.5172702761
Average training loss for Epoch 2 : 2.37068433601
Average training loss for Epoch 3 : 2.27533404217
Average training loss for Epoch 4 : 2.20167231745
It took 537.6112766265869 seconds to train for 5 epochs.
</code></pre></div></div>

<p>So much for that idea. Our custom cell took almost twice as long to train and seems to perform worse than a standard GRU cell.</p>

<h3 id="adding-dropout">Adding Dropout</h3>

<p>Adding features like dropout to the network is easy: we figure out where they belong and drop them in.</p>

<p>Dropout belongs <em>in between layers, not on the state or in intra-cell connections</em>. See <a href="https://arxiv.org/pdf/1409.2329.pdf">Zaremba et al. (2015), Recurrent Neural Network Regularization</a> (“The main idea is to apply the dropout operator only to the non-recurrent connections.”)</p>

<p>Thus, to apply dropout, we need to wrap the input and/or output of <em>each</em> cell. In our RNN implementation using list, we might do something like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rnn_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">rnn_input</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span> <span class="k">for</span> <span class="n">rnn_input</span> <span class="ow">in</span> <span class="n">rnn_inputs</span><span class="p">]</span>
<span class="n">rnn_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">rnn_output</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span> <span class="k">for</span> <span class="n">nn_output</span> <span class="ow">in</span> <span class="n">rnn_outputs</span><span class="p">]</span>
</code></pre></div></div>

<p>In our dynamic_rnn or scan implementations, we might apply dropout directly to the rnn_inputs or rnn_outputs:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rnn_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">rnn_inputd</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
<span class="n">rnn_outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">rnn_outputd</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
</code></pre></div></div>

<p>But what happens when we use <code class="language-plaintext highlighter-rouge">MultiRNNCell</code>? How can we have dropout in between layers like in Zaremba et al. (2015)? The answer is to wrap our base RNN cell with dropout, thereby including it as part of the base cell, similar to how we wrapped our three RNN cells into a single MultiRNNCell above. Tensorflow allows us to do this without writing a new RNNCell by using <code class="language-plaintext highlighter-rouge">tf.nn.rnn_cell.DropoutWrapper</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">DropoutWrapper</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">input_keep_prob</span><span class="o">=</span><span class="n">input_dropout</span><span class="p">,</span> <span class="n">output_keep_prob</span><span class="o">=</span><span class="n">output_dropout</span><span class="p">)</span>
<span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">MultiRNNCell</span><span class="p">([</span><span class="n">cell</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>Note that if we wrap a base cell with dropout and then use it to build a MultiRNNCell, both input dropout and output dropout will be applied between layers (so if both are, say, $ 0.9 $, the dropout in between layers will be $ 0.9 * 0.9 = 0.81 $). If we want equal dropout on all inputs and outputs of a multi-layered RNN, we can use only output or input dropout on the base cell, and then wrap the entire MultiRNNCell with the input or output dropout like so:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">DropoutWrapper</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">input_keep_prob</span><span class="o">=</span><span class="n">global_dropout</span><span class="p">)</span>
<span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">MultiRNNCell</span><span class="p">([</span><span class="n">cell</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">DropoutWrapper</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">output_keep_prob</span><span class="o">=</span><span class="n">global_dropout</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="layer-normalization">Layer normalization</h3>

<p>Layer normalization is a feature that was published just a few days ago by <a href="https://arxiv.org/abs/1607.06450">Lei Ba et al. (2016)</a>, which we can use to improve our RNN. It was inspired by batch normalization, which you can read about and learn how to implement in my post <a href="http://r2rt.com/implementing-batch-normalization-in-tensorflow.html">here</a>. Batch normalization (for feed-forward and convolutional neural networks) and layer normalization (for recurrent neural networks) generally improve training time and achieve better overall performance. In this section, we’ll apply what we’ve learned in this post to implement layer normalization in Tensorflow.</p>

<p>Layer normalization is applied as follows: the initial layer normalization function is applied individually to each training example so as to normalize the output vector of a linear transformation to have a mean of $ 0 $ and a variance of $ 1 $. In math: $ LN_{initial}: v \mapsto \frac{v - \mu_v}{\sqrt{\sigma_v^2 + \epsilon}} $ for some vector $ v $ and some small value of $ \epsilon $ for numerical stability. For some the same reasons we add scale and shift parameters to the initial batch normalization transform (see my <a href="http://r2rt.com/implementing-batch-normalization-in-tensorflow.html">batch normalization post</a> for details), we add scale, $ \alpha $, and shift, $ \beta $, parameters here as well, so that the final layer normalization function is:</p>

<p>$$
LN: v \mapsto \alpha \odot \frac{v - \mu_v}{\sqrt{\sigma_v^2 + \epsilon}} + \beta
$$</p>

<p>Note that $ \odot $ is point-wise multiplication.</p>

<p>To add layer normalization to our network, we first write a function that will layer normalization a 2D tensor along its second dimension:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ln</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">scope</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">):</span>
    <span class="s">""" Layer normalizes a 2D tensor along its second axis """</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">get_shape</span><span class="p">())</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">moments</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">scope</span> <span class="o">=</span> <span class="s">''</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span> <span class="o">+</span> <span class="s">'layer_norm'</span><span class="p">):</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'scale'</span><span class="p">,</span>
                                <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">tensor</span><span class="p">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">1</span><span class="p">]],</span>
                                <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">shift</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'shift'</span><span class="p">,</span>
                                <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">tensor</span><span class="p">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">1</span><span class="p">]],</span>
                                <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">LN_initial</span> <span class="o">=</span> <span class="p">(</span><span class="n">tensor</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span> <span class="o">/</span> <span class="n">tf</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">LN_initial</span> <span class="o">*</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">shift</span>
</code></pre></div></div>

<p>Let’s apply it our layer normalization function as it was applied by Lei Ba et al. (2016) to LSTMs (in their experiments “Teaching machines to read and comprehend” and “Handwriting sequence generation”). Lei Ba et al. apply layer normalization to the output of each gate inside the LSTM cell, which means that we get to take a second shot at writing a new type of RNN cell. We’ll start with Tensorflow’s official code, located <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py">here</a>, and modify it accordingly:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LayerNormalizedLSTMCell</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">RNNCell</span><span class="p">):</span>
    <span class="s">"""
    Adapted from TF's BasicLSTMCell to use Layer Normalization.
    Note that state_is_tuple is always True.
    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_units</span><span class="p">,</span> <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">tanh</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_num_units</span> <span class="o">=</span> <span class="n">num_units</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_forget_bias</span> <span class="o">=</span> <span class="n">forget_bias</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_activation</span> <span class="o">=</span> <span class="n">activation</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">_num_units</span><span class="p">)</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">_num_units</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="s">"""Long short-term memory cell (LSTM)."""</span>
        <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">).</span><span class="n">__name__</span><span class="p">):</span>
            <span class="n">c</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">state</span>

            <span class="c1"># change bias argument to False since LN will add bias via shift
</span>            <span class="n">concat</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">_linear</span><span class="p">([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">h</span><span class="p">],</span> <span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>

            <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">o</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">concat</span><span class="p">)</span>

            <span class="c1"># add layer normalization to each gate
</span>            <span class="n">i</span> <span class="o">=</span> <span class="n">ln</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">scope</span> <span class="o">=</span> <span class="s">'i/'</span><span class="p">)</span>
            <span class="n">j</span> <span class="o">=</span> <span class="n">ln</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">scope</span> <span class="o">=</span> <span class="s">'j/'</span><span class="p">)</span>
            <span class="n">f</span> <span class="o">=</span> <span class="n">ln</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">scope</span> <span class="o">=</span> <span class="s">'f/'</span><span class="p">)</span>
            <span class="n">o</span> <span class="o">=</span> <span class="n">ln</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">scope</span> <span class="o">=</span> <span class="s">'o/'</span><span class="p">)</span>

            <span class="n">new_c</span> <span class="o">=</span> <span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">f</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">_forget_bias</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">*</span>
                   <span class="bp">self</span><span class="p">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">j</span><span class="p">))</span>

            <span class="c1"># add layer_normalization in calculation of new hidden state
</span>            <span class="n">new_h</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">ln</span><span class="p">(</span><span class="n">new_c</span><span class="p">,</span> <span class="n">scope</span> <span class="o">=</span> <span class="s">'new_h/'</span><span class="p">))</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
            <span class="n">new_state</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="n">new_c</span><span class="p">,</span> <span class="n">new_h</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">new_h</span><span class="p">,</span> <span class="n">new_state</span>
</code></pre></div></div>

<p>And that’s it! Let’s try this out.</p>

<h3 id="final-model">Final model</h3>
<p>At this point, we’ve covered all of the graph modifications we planned to cover, so here is our final model, which allows for dropout and layer normalized LSTM cells:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_graph</span><span class="p">(</span>
    <span class="n">cell_type</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">num_weights_for_custom_cell</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">state_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">build_with_dropout</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">):</span>

    <span class="n">reset_graph</span><span class="p">()</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'input_placeholder'</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'labels_placeholder'</span><span class="p">)</span>

    <span class="n">dropout</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>

    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'embedding_matrix'</span><span class="p">,</span> <span class="p">[</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">state_size</span><span class="p">])</span>

    <span class="n">rnn_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">cell_type</span> <span class="o">==</span> <span class="s">'Custom'</span><span class="p">:</span>
        <span class="n">cell</span> <span class="o">=</span> <span class="n">CustomCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">num_weights_for_custom_cell</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">cell_type</span> <span class="o">==</span> <span class="s">'GRU'</span><span class="p">:</span>
        <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">GRUCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">cell_type</span> <span class="o">==</span> <span class="s">'LSTM'</span><span class="p">:</span>
        <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">cell_type</span> <span class="o">==</span> <span class="s">'LN_LSTM'</span><span class="p">:</span>
        <span class="n">cell</span> <span class="o">=</span> <span class="n">LayerNormalizedLSTMCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">BasicRNNCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">build_with_dropout</span><span class="p">:</span>
        <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">DropoutWrapper</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">input_keep_prob</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">cell_type</span> <span class="o">==</span> <span class="s">'LSTM'</span> <span class="ow">or</span> <span class="n">cell_type</span> <span class="o">==</span> <span class="s">'LN_LSTM'</span><span class="p">:</span>
        <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">MultiRNNCell</span><span class="p">([</span><span class="n">cell</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">MultiRNNCell</span><span class="p">([</span><span class="n">cell</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">build_with_dropout</span><span class="p">:</span>
        <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">DropoutWrapper</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">output_keep_prob</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

    <span class="n">init_state</span> <span class="o">=</span> <span class="n">cell</span><span class="p">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">rnn_outputs</span><span class="p">,</span> <span class="n">final_state</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">dynamic_rnn</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">rnn_inputs</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">init_state</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">'softmax'</span><span class="p">):</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'W'</span><span class="p">,</span> <span class="p">[</span><span class="n">state_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">])</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'b'</span><span class="p">,</span> <span class="p">[</span><span class="n">num_classes</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>

    <span class="c1">#reshape rnn_outputs and y
</span>    <span class="n">rnn_outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rnn_outputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">state_size</span><span class="p">])</span>
    <span class="n">y_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">rnn_outputs</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>

    <span class="n">predictions</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y_reshaped</span><span class="p">))</span>
    <span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">).</span><span class="n">minimize</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span>
        <span class="n">init_state</span> <span class="o">=</span> <span class="n">init_state</span><span class="p">,</span>
        <span class="n">final_state</span> <span class="o">=</span> <span class="n">final_state</span><span class="p">,</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="n">total_loss</span><span class="p">,</span>
        <span class="n">train_step</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">,</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">,</span>
        <span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">Saver</span><span class="p">()</span>
    <span class="p">)</span>
</code></pre></div></div>

<p>Let’s compare the GRU, LSTM and LN_LSTM after training each for 20 epochs using 80 step sequences.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">g</span> <span class="o">=</span> <span class="n">build_graph</span><span class="p">(</span><span class="n">cell_type</span><span class="o">=</span><span class="s">'GRU'</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">losses</span> <span class="o">=</span> <span class="n">train_network</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">save</span><span class="o">=</span><span class="s">"saves/GRU_20_epochs"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"It took"</span><span class="p">,</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="s">"seconds to train for 20 epochs."</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"The average loss on the final epoch was:"</span><span class="p">,</span> <span class="n">losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>It took 1051.6652357578278 seconds to train for 20 epochs.
The average loss on the final epoch was: 1.75318197903
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">g</span> <span class="o">=</span> <span class="n">build_graph</span><span class="p">(</span><span class="n">cell_type</span><span class="o">=</span><span class="s">'LSTM'</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">losses</span> <span class="o">=</span> <span class="n">train_network</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">save</span><span class="o">=</span><span class="s">"saves/LSTM_20_epochs"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"It took"</span><span class="p">,</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="s">"seconds to train for 20 epochs."</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"The average loss on the final epoch was:"</span><span class="p">,</span> <span class="n">losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>It took 614.4890048503876 seconds to train for 20 epochs.
The average loss on the final epoch was: 2.02813237837
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">g</span> <span class="o">=</span> <span class="n">build_graph</span><span class="p">(</span><span class="n">cell_type</span><span class="o">=</span><span class="s">'LN_LSTM'</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">losses</span> <span class="o">=</span> <span class="n">train_network</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">save</span><span class="o">=</span><span class="s">"saves/LN_LSTM_20_epochs"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"It took"</span><span class="p">,</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="s">"seconds to train for 20 epochs."</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"The average loss on the final epoch was:"</span><span class="p">,</span> <span class="n">losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>It took 3867.550405740738 seconds to train for 20 epochs.
The average loss on the final epoch was: 1.71850851623
</code></pre></div></div>

<p>It looks like the layer normalized LSTM just managed to edge out the GRU in the last few epochs, though the increase in training time hardly seems worth it (perhaps my implementation could be improved?). It would be interesting to see how they would perform on a validation or test set and also to try out a layer normalized GRU. For now, let’s use the GRU to generate some text.</p>

<h3 id="generating-text">Generating text</h3>

<p>To generate text, were going to rebuild the graph so as to accept a single character at a time and restore our saved model. We’ll give the network a single character prompt, grab its predicted probability distribution for the next character, use that distribution to pick the next character, and repeat. When picking the next character, our <code class="language-plaintext highlighter-rouge">generate_characters</code> function can be set to use the whole probability distribution (default), or be forced to pick one of the top n most likely characters in the distribution. The latter option should obtain more English-like results.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_characters</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">,</span> <span class="n">num_chars</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="s">'A'</span><span class="p">,</span> <span class="n">pick_top_chars</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="s">""" Accepts a current character, initial state"""</span>

    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
        <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">initialize_all_variables</span><span class="p">())</span>
        <span class="n">g</span><span class="p">[</span><span class="s">'saver'</span><span class="p">].</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">)</span>

        <span class="n">state</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">current_char</span> <span class="o">=</span> <span class="n">vocab_to_idx</span><span class="p">[</span><span class="n">prompt</span><span class="p">]</span>
        <span class="n">chars</span> <span class="o">=</span> <span class="p">[</span><span class="n">current_char</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_chars</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">g</span><span class="p">[</span><span class="s">'x'</span><span class="p">]:</span> <span class="p">[[</span><span class="n">current_char</span><span class="p">]],</span> <span class="n">g</span><span class="p">[</span><span class="s">'init_state'</span><span class="p">]:</span> <span class="n">state</span><span class="p">}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">g</span><span class="p">[</span><span class="s">'x'</span><span class="p">]:</span> <span class="p">[[</span><span class="n">current_char</span><span class="p">]]}</span>

            <span class="n">preds</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">([</span><span class="n">g</span><span class="p">[</span><span class="s">'preds'</span><span class="p">],</span><span class="n">g</span><span class="p">[</span><span class="s">'final_state'</span><span class="p">]],</span> <span class="n">feed_dict</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">pick_top_chars</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
                <span class="n">p</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">p</span><span class="p">)[:</span><span class="o">-</span><span class="n">pick_top_chars</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">p</span> <span class="o">=</span> <span class="n">p</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
                <span class="n">current_char</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">current_char</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">preds</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">chars</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_char</span><span class="p">)</span>

    <span class="n">chars</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">idx_to_vocab</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">chars</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">""</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">chars</span><span class="p">))</span>
    <span class="k">return</span><span class="p">(</span><span class="s">""</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">chars</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">g</span> <span class="o">=</span> <span class="n">build_graph</span><span class="p">(</span><span class="n">cell_type</span><span class="o">=</span><span class="s">'LN_LSTM'</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">generate_characters</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="s">"saves/LN_LSTM_20_epochs"</span><span class="p">,</span> <span class="mi">750</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="s">'A'</span><span class="p">,</span> <span class="n">pick_top_chars</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ATOOOS

 UIEAOUYOUZZZZZZUZAAAYAYf n fsflflrurctuateot t ta's  a  wtutss ESGNANO:
Whith then, a do makes and them and to sees,
I wark on this ance may string take thou honon
To sorriccorn of the bairer, whither, all
I'd see if yiust the would a peid.

LARYNGLe:
To would she troust they fould.

PENMES:
Thou she so the havin to my shald woust of
As tale we they all my forder have
As to say heant thy wansing thag and
Whis it thee shath his breact, I be and might, she
Tirs you desarvishensed and see thee: shall,
What he hath with that is all time,
And sen the have would be sectiens, way thee,
They are there to man shall with me to the mon,
And mere fear would be the balte, as time an at
And the say oun touth, thy way womers thee.
</code></pre></div></div>

<p>You can see that this network has learned something. It’s definitely not random, though there is a bit of a warm up at the beginning (the state starts at 0). I was expecting something a bit better, however, given <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/#shakespeare">Karpathy’s Shakespeare results</a>. His model used more data, a state_size of 512, and was trained quite a bit longer than this one. Let’s see if we can match that. I couldn’t find a suitable premade dataset, so I had to make one myself: I concatenated the scripts from the Star Wars movies, the Star Trek movies, Tarantino and the Matrix. The final file size is 3.3MB, which is a bit smaller than the full works of William Shakespeare. Let’s load these up and try this again, with a larger state size:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
Load new data
"""</span>

<span class="n">file_url</span> <span class="o">=</span> <span class="s">'https://gist.githubusercontent.com/spitis/59bfafe6966bfe60cc206ffbb760269f/'</span><span class="o">+</span>\
<span class="s">'raw/030a08754aada17cef14eed6fac7797cda830fe8/variousscripts.txt'</span>
<span class="n">file_name</span> <span class="o">=</span> <span class="s">'variousscripts.txt'</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="n">file_name</span><span class="p">):</span>
    <span class="n">urllib</span><span class="p">.</span><span class="n">request</span><span class="p">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="n">file_url</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_name</span><span class="p">,</span><span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">raw_data</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Data length:"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">raw_data</span><span class="p">))</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">raw_data</span><span class="p">)</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">idx_to_vocab</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">))</span>
<span class="n">vocab_to_idx</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">idx_to_vocab</span><span class="p">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">idx_to_vocab</span><span class="p">.</span><span class="n">keys</span><span class="p">()))</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocab_to_idx</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">raw_data</span><span class="p">]</span>
<span class="k">del</span> <span class="n">raw_data</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Data length: 3299132
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">g</span> <span class="o">=</span> <span class="n">build_graph</span><span class="p">(</span><span class="n">cell_type</span><span class="o">=</span><span class="s">'GRU'</span><span class="p">,</span>
                <span class="n">num_steps</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span>
                <span class="n">state_size</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
                <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
                <span class="n">num_classes</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
                <span class="n">learning_rate</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">losses</span> <span class="o">=</span> <span class="n">train_network</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">save</span><span class="o">=</span><span class="s">"saves/GRU_30_epochs_variousscripts"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"It took"</span><span class="p">,</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="s">"seconds to train for 30 epochs."</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"The average loss on the final epoch was:"</span><span class="p">,</span> <span class="n">losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>It took 4877.8002140522 seconds to train for 30 epochs.
The average loss on the final epoch was: 0.726858645461
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">g</span> <span class="o">=</span> <span class="n">build_graph</span><span class="p">(</span><span class="n">cell_type</span><span class="o">=</span><span class="s">'GRU'</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">state_size</span> <span class="o">=</span> <span class="mi">512</span><span class="p">)</span>
<span class="n">generate_characters</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="s">"saves/GRU_30_epochs_variousscripts"</span><span class="p">,</span> <span class="mi">750</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="s">'D'</span><span class="p">,</span> <span class="n">pick_top_chars</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DENT'SUEENCK

Bartholomew of the TIE FIGHTERS are stunned. There is a crowd and armored
switcheroos.

PICARD
(continuing)
Couns two dim is tired. In order to the sentence...

The sub    bottle appears on the screen into a small shuttle shift of the
ceiling. The DAMBA FETT splash fires and matches them into the top, transmit to stable high above upon their statels,
falling from an alien shaft.

ANAKIN and OBI-WAN stand next to OBI-WAN down the control plate of smoke at the TIE fighter. They stare at the centre of the station loose into a comlink cover -- comes up to the General, the GENERAL HUNTAN AND FINNFURMBARD from the PICADOR to a beautiful Podracisly.

ENGINEER
Naboo from an army seventy medical
security team area re-weilergular.

EXT.
</code></pre></div></div>

<p>Not sure these are that much better than before, but it’s sort of readable?</p>

<h3 id="conclusion">Conclusion</h3>
<p>In this post, we used a character sequence generation task to learn how to use Tensorflow’s scan and dynamic_rnn functions, how to use advanced RNN cells and stack multiple RNNs, and how to add features to our RNN like dropout and layer normalization. In the next post, we will use a machine translation task to look at handling variable length sequences and building RNN encoders and decoders.</p>

  </div>

<!----><script defer src="https://cdn.commento.io/js/commento.js"></script>
  <noscript>Please enable JavaScript to load the comments.</noscript>
  <div id="commento"></div>
<!-- comments.html checks if comments are enabled for the post -->

  <a class="u-url" href="/jekyll/tensorflow/neuralnetworks/2022/01/04/recurrent-neural-networks-in-tensorflow-ii.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Verisk Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Verisk Blog</li>
<li><a class="u-email" href="mailto:your-email@example.com">your-email@example.com</a></li>
</ul>
      </div>

      <div class="footer-col footer-col-2">
<ul class="social-media-list">
<li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li>
<li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li>
</ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
